{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67004199-d104-4212-8b4e-bb36b81d7ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.agcd_agg_functions' from 'c:\\\\Users\\\\jake.allen.ALLUVIUMQLD\\\\Documents\\\\Repos\\\\climate_health_reference_manual\\\\utils\\\\agcd_agg_functions.py'>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import utils.agcd_agg_functions as agg\n",
    "from importlib import reload\n",
    "from shapely.geometry import mapping, box\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.features import geometry_mask\n",
    "import dill\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns',  None)\n",
    "\n",
    "reload(agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630fb415-784c-4169-95d2-39e28019ea4e",
   "metadata": {},
   "source": [
    "### Paths to NetCDF files\n",
    "We load everything into xarray dataset definitions for easy iterations later. Note that no data is being held in memory yet thanks to dask. This only happens once computations are triggered later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d5026d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build path dictionary\n",
    "clim_fold = 'E:/Jake_ClimateRasters'\n",
    "\n",
    "def nested_dict():\n",
    "    return defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "path_dict = defaultdict(lambda: defaultdict(nested_dict))\n",
    "\n",
    "models = ['ACCESS-ESM1-5']\n",
    "ssps = ['ssp245', 'ssp370']\n",
    "epochs = ['mid', 'late']\n",
    "vars = ['hurs', 'tasmax', 'tasmin']\n",
    "\n",
    "# assign model paths\n",
    "for model in models:\n",
    "    for ssp in ssps:\n",
    "        for epoch in epochs:\n",
    "            thisepoch = '2035-2064' if epoch == 'mid' else '2070-2099'\n",
    "            for thisvar in vars:\n",
    "                thispath = Path(f'{clim_fold}/{model}/{ssp}/{thisvar}/AUS-11/{thisepoch}')\n",
    "                ncdfs = []\n",
    "                for path in thispath.rglob('*.nc'):\n",
    "                    ncdfs.append(path)\n",
    "                path_dict[model][ssp][epoch][thisvar] = ncdfs\n",
    "\n",
    "# assign historical observation paths\n",
    "for thisvar in vars:\n",
    "    thispath = Path(f'{clim_fold}/Historical/{thisvar}')\n",
    "    ncdfs = []\n",
    "    for path in thispath.rglob('*.nc'):\n",
    "        ncdfs.append(path)\n",
    "    path_dict['Historical'][thisvar] = ncdfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd83fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load districts\n",
    "\n",
    "districts = gpd.read_file('Inputs/health_district_merged.json')\n",
    "\n",
    "# subset districts if necessary\n",
    "sub = districts[districts['health_district_name'].isin(['Northern NSW', 'Western Sydney', 'Eyre and Far North'])].copy().to_crs('EPSG:4326')\n",
    "\n",
    "# get bounding box\n",
    "minx, miny, maxx, maxy = sub.total_bounds\n",
    "\n",
    "bbox_geom = box(minx, miny, maxx, maxy)\n",
    "bbox = [mapping(bbox_geom)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2c9523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ncdfs into dictionary for easy wrangling, subsetting to bounding box and chunking by time = 365\n",
    "# use load_var helper function\n",
    "\n",
    "# initialise empty dictionary\n",
    "data_dict = defaultdict(lambda: defaultdict(nested_dict))\n",
    "\n",
    "# load model data\n",
    "for model in models:\n",
    "    for ssp in ssps:\n",
    "        for epoch in epochs:\n",
    "            ds = xr.Dataset({\n",
    "                thisvar: agg.load_var(path_dict[model][ssp][epoch][thisvar], thisvar, bbox)\n",
    "                for thisvar in vars\n",
    "            })\n",
    "            data_dict[model][ssp][epoch] = ds\n",
    "\n",
    "\n",
    "# Load all historical variables into a dict\n",
    "# historical data has misaligned time coordinates, causing issues when combining into a single dataset\n",
    "hist_vars = {}\n",
    "for thisvar in vars:\n",
    "    ds = agg.load_var(path_dict['Historical'][thisvar], thisvar, bbox, chunk=False)\n",
    "\n",
    "    # Force matching time coordinates\n",
    "    if 'time' in hist_vars:\n",
    "        ds['time'] = hist_vars['time']\n",
    "    else:\n",
    "        hist_vars['time'] = ds['time']\n",
    "\n",
    "    hist_vars[thisvar] = ds\n",
    "\n",
    "# Drop the saved 'time' array from the dict\n",
    "hist_vars.pop('time')\n",
    "\n",
    "# Now build the dataset\n",
    "data_dict['Historical'] = xr.Dataset(hist_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493b34a-f138-48e6-8c7a-1739bf0139ac",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a129424b",
   "metadata": {},
   "source": [
    "#### Define mean temperature and heat index\n",
    "These metrics need to be calculated for each day throughout the time series. Here we are just defining the metrics, not triggering the dask computation yet. \n",
    "Dill is used as a checkpointing mechanism - saving interim results after each model iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c61ac8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each dataset to define mean temp and heat index. Heat index is using a function pulled from our utils functions file.\n",
    "\n",
    "for model in models:\n",
    "    for ssp in ssps:\n",
    "        for epoch in epochs:\n",
    "                ds = data_dict[model][ssp][epoch]\n",
    "                # mean temp\n",
    "                ds['tas'] = (ds['tasmax'] + ds['tasmin']) / 2\n",
    "                # heat index\n",
    "                ds['hi'] = agg.calculate_heat_index(ds['tasmax'], ds['hurs'])\n",
    "\n",
    "hist = data_dict['Historical']\n",
    "hist['tas'] = (hist['tasmax'] + hist['tasmin']) / 2\n",
    "hist['hi'] = agg.calculate_heat_index(hist['tasmax'], hist['hurs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e19e026",
   "metadata": {},
   "source": [
    "##### Average hot days (thresholds) per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7f7fb2-0af8-4e5c-bf18-8e052b644932",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_days_dict = defaultdict(lambda: defaultdict(nested_dict))\n",
    "\n",
    "for model in models:\n",
    "    for ssp in ssps:\n",
    "        for epoch in epochs:\n",
    "            ds = data_dict[model][ssp][epoch]\n",
    "            hot_days_dict[model][ssp][epoch] = {\n",
    "                thresh: agg.calculate_avg_hot_days(ds, threshold=thresh).compute()\n",
    "                for thresh in [35,40]\n",
    "            }\n",
    "    agg.checkpoint_update('netcdf-wrangle-dicts.pkl', {\n",
    "        'hot_days': hot_days_dict\n",
    "    })\n",
    "\n",
    "hot_days_dict['Historical']['Historical']['Historical'] = {\n",
    "        thresh: agg.calculate_avg_hot_days(data_dict['Historical'], threshold=thresh).compute()\n",
    "        for thresh in [35, 40]\n",
    "}\n",
    "agg.checkpoint_update('netcdf-wrangle-dicts.pkl', {\n",
    "        'hot_days': hot_days_dict\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b0a700",
   "metadata": {},
   "source": [
    "#### Percentile temps maximum 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23170f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict to store ncdf results to\n",
    "pc_max_95_dict = defaultdict(lambda: defaultdict(nested_dict))\n",
    "\n",
    "for model in models:\n",
    "    for ssp in ssps:\n",
    "        for epoch in epochs:\n",
    "            ds = data_dict[model][ssp][epoch]\n",
    "            pc_95_max = agg.calculate_percentile(ds, 'tasmax').compute()\n",
    "            pc_max_95_dict[model][ssp][epoch] = pc_95_max\n",
    "    agg.checkpoint_update('netcdf-wrangle-dicts.pkl', {\n",
    "        'pc_max_95': pc_max_95_dict\n",
    "    })\n",
    "\n",
    "# historical\n",
    "ds = data_dict['Historical']\n",
    "pc_95_max = agg.calculate_percentile(ds, 'tasmax').compute()\n",
    "pc_max_95_dict['Historical']['Historical']['Historical'] = pc_95_max\n",
    "agg.checkpoint_update('netcdf-wrangle-dicts.pkl', {\n",
    "        'pc_max_95': pc_max_95_dict\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aecb91",
   "metadata": {},
   "source": [
    "#### Percentile temps minimum, 95%\n",
    "can probably turn this into a loop with max, but sometimes useful to run seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac73a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict to store ncdf results to\n",
    "pc_min_95_dict = defaultdict(lambda: defaultdict(nested_dict))\n",
    "\n",
    "for model in models:\n",
    "    for ssp in ssps:\n",
    "        for epoch in epochs:\n",
    "            ds = data_dict[model][ssp][epoch]\n",
    "            pc_95_min = agg.calculate_percentile(ds, 'tasmin').compute()\n",
    "            pc_min_95_dict[model][ssp][epoch] = pc_95_min\n",
    "    agg.checkpoint_update('netcdf-wrangle-dicts.pkl', {\n",
    "        'pc_min_95': pc_min_95_dict\n",
    "    })\n",
    "\n",
    "# historical\n",
    "ds = data_dict['Historical']\n",
    "pc_95_min = agg.calculate_percentile(ds, 'tasmin').compute()\n",
    "pc_min_95_dict['Historical']['Historical']['Historical'] = pc_95_min\n",
    "agg.checkpoint_update('netcdf-wrangle-dicts.pkl', {\n",
    "        'pc_min_95': pc_min_95_dict\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7625999f",
   "metadata": {},
   "source": [
    "#### Heat Index 95% - Not needed currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict to store ncdf results to\n",
    "pc_hi_95_dict = defaultdict(lambda: defaultdict(nested_dict))\n",
    "\n",
    "for model in models:\n",
    "    for ssp in ssps:\n",
    "        for epoch in epochs:\n",
    "            ds = data_dict[model][ssp][epoch]\n",
    "            pc_95_hi = agg.calculate_percentile(ds, 'hi').compute()\n",
    "            pc_hi_95_dict[model][ssp][epoch] = pc_95_hi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d53fba",
   "metadata": {},
   "source": [
    "#### Heat Index mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669eef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_hi_dict = defaultdict(lambda: defaultdict(nested_dict))\n",
    "\n",
    "# mean heat index for whole period\n",
    "for model in models:\n",
    "    for ssp in ssps:\n",
    "        for epoch in epochs:\n",
    "            ds = data_dict[model][ssp][epoch]\n",
    "            hi_mean = ds['hi'].mean(dim='time').compute()\n",
    "            mean_hi_dict[model][ssp][epoch] = hi_mean\n",
    "    agg.checkpoint_update('netcdf-wrangle-dicts.pkl', {\n",
    "        'mean_hi': mean_hi_dict\n",
    "    })\n",
    "\n",
    "# historical\n",
    "ds = data_dict['Historical']\n",
    "hi_mean = ds['hi'].mean(dim='time').compute()\n",
    "mean_hi_dict['Historical']['Historical']['Historical']  = hi_mean\n",
    "agg.checkpoint_update('netcdf-wrangle-dicts.pkl', {\n",
    "        'mean_hi': mean_hi_dict\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccecf0e",
   "metadata": {},
   "source": [
    "##### Mean temp mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bdeaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mean_dict = defaultdict(lambda: defaultdict(nested_dict))\n",
    "\n",
    "# mean temperature for the whole period\n",
    "for model in models:\n",
    "    for ssp in ssps:\n",
    "        for epoch in epochs:\n",
    "            ds = data_dict[model][ssp][epoch]\n",
    "            mean_mean = ds['tas'].mean(dim='time').compute()\n",
    "            mean_mean_dict[model][ssp][epoch] = mean_mean\n",
    "    agg.checkpoint_update('netcdf-wrangle-dicts.pkl', {\n",
    "        'mean_mean': mean_mean_dict\n",
    "    })\n",
    "\n",
    "# historical\n",
    "ds = data_dict['Historical']\n",
    "mean_mean = ds['tas'].mean(dim='time').compute()\n",
    "mean_mean_dict['Historical']['Historical']['Historical'] = mean_mean\n",
    "agg.checkpoint_update('netcdf-wrangle-dicts.pkl', {\n",
    "        'mean_mean': mean_mean_dict\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ba0b3",
   "metadata": {},
   "source": [
    "#### 95th percentile of historical mean temp for EHF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fb1885",
   "metadata": {},
   "source": [
    "#### Excess Heat Factor (EHF) (Heatwaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jake.allen.ALLUVIUMQLD\\AppData\\Local\\anaconda3\\envs\\climate-env\\Lib\\site-packages\\dask\\array\\core.py:5092: PerformanceWarning: Increasing number of chunks by factor of 10\n",
      "  result = blockwise(\n",
      "C:\\Users\\jake.allen.ALLUVIUMQLD\\AppData\\Local\\anaconda3\\envs\\climate-env\\Lib\\site-packages\\dask\\array\\core.py:5092: PerformanceWarning: Increasing number of chunks by factor of 10\n",
      "  result = blockwise(\n",
      "C:\\Users\\jake.allen.ALLUVIUMQLD\\AppData\\Local\\anaconda3\\envs\\climate-env\\Lib\\site-packages\\dask\\array\\core.py:5092: PerformanceWarning: Increasing number of chunks by factor of 10\n",
      "  result = blockwise(\n",
      "C:\\Users\\jake.allen.ALLUVIUMQLD\\AppData\\Local\\anaconda3\\envs\\climate-env\\Lib\\site-packages\\dask\\array\\core.py:5092: PerformanceWarning: Increasing number of chunks by factor of 10\n",
      "  result = blockwise(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10957,), (50, 32), (50, 50, 50, 50, 24))\n"
     ]
    }
   ],
   "source": [
    "# compute 95th percentile of historical mean temperature\n",
    "mean_95_hist = agg.calculate_percentile(data_dict['Historical'], 'tas', 95)\n",
    "\n",
    "hw_dict = defaultdict(lambda: defaultdict(nested_dict))\n",
    "\n",
    "# mean temperature for the whole period\n",
    "for model in models:\n",
    "    for ssp in ssps:\n",
    "        for epoch in epochs:\n",
    "            ds = data_dict[model][ssp][epoch]\n",
    "            ehf = agg.calculate_ehf(ds['tas'], mean_95_hist)\n",
    "            ehf = ehf.chunk({'lat': 50, 'lon': 50, 'time': -1})\n",
    "            hw = agg.summarise_heatwaves(ehf)\n",
    "            hw = {k: v.compute() for k, v in hw.items()}\n",
    "            hw_dict[model][ssp][epoch] = hw\n",
    "    agg.checkpoint_update('netcdf-wrangle-dicts.pkl', {\n",
    "        'heatwaves': hw_dict\n",
    "    })\n",
    "\n",
    "# historical\n",
    "ds = data_dict['Historical']\n",
    "ehf = agg.calculate_ehf(ds['tas'], mean_95_hist)\n",
    "ehf = ehf.chunk({'lat': 50, 'lon': 50, 'time': -1})\n",
    "print(ehf.chunks)\n",
    "hw = agg.summarise_heatwaves(ehf)\n",
    "hw = {k: v.compute() for k, v in hw.items()}\n",
    "hw_dict['Historical']['Historical']['Historical'] = hw\n",
    "agg.checkpoint_update('netcdf-wrangle-dicts.pkl', {\n",
    "        'heatwaves': hw_dict\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d9ea60",
   "metadata": {},
   "source": [
    "#### Combine metric dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14d08178",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dicts = {\n",
    "    'heatwaves': hw_dict,\n",
    "    'pc_max_95': pc_max_95_dict,\n",
    "    'pc_min_95': pc_min_95_dict,\n",
    "    'mean_heat_index': mean_hi_dict,\n",
    "    'mean_temp': mean_mean_dict,\n",
    "    'avg_hot_days': hot_days_dict\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a3efe1",
   "metadata": {},
   "source": [
    "#### Population weighting - resample and align population raster to netcdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f46531a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open population raster\n",
    "pop = rioxarray.open_rasterio(\"Inputs/australian_pop_grid_2024/apg24e_1_0_0.tif\").squeeze() # remove band dimension\n",
    "\n",
    "# ensure correct projections are assinged (GDA94 for pop grid, WGS 84 for netcdfs)\n",
    "pop.rio.write_crs(\"EPSG:3577\", inplace=True)\n",
    "# convert -1 to NaN and set as no data\n",
    "pop = pop.where(pop != -1, np.nan)\n",
    "pop = pop.rio.write_nodata(np.nan, inplace=False)\n",
    "\n",
    "ref = data_dict['Historical']['hurs'].isel(time=0)\n",
    "ref.rio.set_spatial_dims(x_dim='lon', y_dim='lat', inplace=True)\n",
    "\n",
    "# reproject and resample, summing values as the resample method\n",
    "pop_aligned = pop.rio.reproject_match(\n",
    "    ref,\n",
    "    resampling=Resampling.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b7764c",
   "metadata": {},
   "source": [
    "#### Build metrics dataframe - applying population weighting to health districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd578ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>state</th>\n",
       "      <th>health_district_name</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>district_id</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Northern NSW</td>\n",
       "      <td>11.494546</td>\n",
       "      <td>1.989182</td>\n",
       "      <td>NSW_7</td>\n",
       "      <td>MULTIPOLYGON (((153.63873 -28.6361, 153.63869 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Western Sydney</td>\n",
       "      <td>7.340251</td>\n",
       "      <td>0.686059</td>\n",
       "      <td>NSW_9</td>\n",
       "      <td>POLYGON ((150.98407 -33.38804, 150.98402 -33.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>Eyre and Far North</td>\n",
       "      <td>57.676729</td>\n",
       "      <td>46.122717</td>\n",
       "      <td>SA_5</td>\n",
       "      <td>MULTIPOLYGON (((135.95444 -35.00627, 135.9545 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    OBJECTID            state health_district_name  Shape_Length  Shape_Area  \\\n",
       "6          7  New South Wales         Northern NSW     11.494546    1.989182   \n",
       "8          9  New South Wales       Western Sydney      7.340251    0.686059   \n",
       "38        39  South Australia   Eyre and Far North     57.676729   46.122717   \n",
       "\n",
       "   district_id                                           geometry  \n",
       "6        NSW_7  MULTIPOLYGON (((153.63873 -28.6361, 153.63869 ...  \n",
       "8        NSW_9  POLYGON ((150.98407 -33.38804, 150.98402 -33.3...  \n",
       "38        SA_5  MULTIPOLYGON (((135.95444 -35.00627, 135.9545 ...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve health districts and reproject to match.\n",
    "\n",
    "all_districts = gpd.read_file('Inputs/health_district_merged.json')\n",
    "\n",
    "# subset\n",
    "districts = all_districts[all_districts['health_district_name'].isin(['Northern NSW', 'Western Sydney', 'Eyre and Far North'])].copy()\n",
    "\n",
    "districts.to_crs(pop_aligned.rio.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39194eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare geometries and affine transformation for zonal statistics\n",
    "shapes = [mapping(geom) for geom in districts.geometry]\n",
    "affine = pop_aligned.rio.transform()\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Loop through each metric dictionary\n",
    "for metric_name, metric_dict in metric_dicts.items():\n",
    "    for model in metric_dict:\n",
    "        for ssp in metric_dict[model]:\n",
    "            for epoch in metric_dict[model][ssp]:\n",
    "                data = metric_dict[model][ssp][epoch]\n",
    "\n",
    "                # Handle metrics that include subcategories (e.g., hot days by threshold)\n",
    "                if isinstance(data, dict):\n",
    "                    for submetric, subdata in data.items():\n",
    "                        # Compute zonal weighted means for each district\n",
    "                        weighted_means = agg.zonal_weighted_mean(subdata, pop_aligned, shapes, affine)\n",
    "                        for i, v in enumerate(weighted_means):\n",
    "                            rows.append({\n",
    "                                'district_id': districts.iloc[i]['district_id'],  # Unique identifier\n",
    "                                'district_name': districts.iloc[i]['health_district_name'],        # name\n",
    "                                'model': model,\n",
    "                                'ssp': ssp,\n",
    "                                'epoch': epoch,\n",
    "                                'metric': f'{metric_name}_{submetric}',            # e.g. hot_days_35\n",
    "                                'value': v\n",
    "                            })\n",
    "                else:\n",
    "                    # Handle single-layer metric values (no subcategories)\n",
    "                    weighted_means = agg.zonal_weighted_mean(data, pop_aligned, shapes, affine)\n",
    "                    for i, v in enumerate(weighted_means):\n",
    "                        rows.append({\n",
    "                            'district_id': districts.iloc[i]['district_id'],\n",
    "                            'district_name': districts.iloc[i]['health_district_name'],\n",
    "                            'model': model,\n",
    "                            'ssp': ssp,\n",
    "                            'epoch': epoch,\n",
    "                            'metric': metric_name,\n",
    "                            'value': v\n",
    "                        })\n",
    "\n",
    "# Final combined long-format DataFrame\n",
    "results_df = pd.DataFrame(rows)\n",
    "results_df.to_csv('heat_humidity_weighted_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e22f048",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('heat_humidity_weighted_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c3f7b3",
   "metadata": {},
   "source": [
    "### Save and reload workspace dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fb812ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save key processed outputs to workspace\n",
    "\n",
    "with open('netcdf-wrangle-workspace.pkl', 'wb') as f:\n",
    "    dill.dump({\n",
    "        'metrics': metric_dicts\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d482fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload\n",
    "\n",
    "with open('netcdf-wrangle-workspace.pkl', 'rb') as f:\n",
    "    saved = dill.load(f)\n",
    "    \n",
    "metric_dicts = saved['metrics']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
